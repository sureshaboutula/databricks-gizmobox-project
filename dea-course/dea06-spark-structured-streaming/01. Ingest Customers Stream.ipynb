{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9e2bc83-f9f9-4cec-9be6-b984f8116796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Stream Customers Data From Cloud Files to Delta Lake  \n",
    "1. Read files from cloud storage using DataStreamReader API\n",
    "2. Transform the dataframe to add the following columns\n",
    "- file_path: Cloud File Path\n",
    "- ingestion date: Current Timestamp\n",
    "3. Write the Transformed data stream to Delta Lake Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f751aa67-6746-4bb9-b0f8-d0bbed546ece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Read files from cloud storage using DataStreamReader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "876f888c-775b-426d-9e05-351938906da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DateType\n",
    "\n",
    "customers_schema = StructType(fields = [\n",
    "    StructField(\"customer_id\", IntegerType()),\n",
    "    StructField(\"customer_name\", StringType()),\n",
    "    StructField(\"date_of_birth\", DateType()),\n",
    "    StructField(\"telephone\", StringType()),\n",
    "    StructField(\"email\", StringType()),\n",
    "    StructField(\"member_since\", DateType()),\n",
    "    StructField(\"created_timestamp\", TimestampType())\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "181fc27b-8e40-4798-aeb5-4c171e2f103b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_df = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(customers_schema) \\\n",
    "    .load('/Volumes/gizmobox/landing/operational_data/customers_stream/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4e1ddae-248b-4bd2-957d-d91216238b99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Transform the dataframe to add the following columns\n",
    "- file_path: Cloud File Path\n",
    "- ingestion date: Current Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed096513-634b-4bc2-80eb-8ee2388f9540",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "customers_transformed_df = customers_df.withColumn(\"file_path\", col(\"_metadata.file_path\")) \\\n",
    "                                        .withColumn(\"ingestion_date\", current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "627e17e6-a066-4095-94eb-6bd409f38262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Write the Transformed data stream to Delta Lake Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71597ff1-156c-4677-84dd-780f297bbbeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_query = customers_transformed_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "        .option(\"checkpointLocation\", \"/Volumes/gizmobox/landing/operational_data/customers_stream/_checkpoint_stream\") \\\n",
    "        .toTable(\"gizmobox.bronze.customers_stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5213650a-d8f9-4272-8f23-31a66ceb4b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766dd403-124c-418e-a5bb-5c547e0d2f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select *\n",
    "from gizmobox.bronze.customers_stream;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3fad1e8-c7e5-446e-85d3-6390fb35aeaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7039963381109851,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01. Ingest Customers Stream",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
