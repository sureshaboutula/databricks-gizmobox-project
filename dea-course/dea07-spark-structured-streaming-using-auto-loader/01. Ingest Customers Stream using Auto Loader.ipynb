{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9e2bc83-f9f9-4cec-9be6-b984f8116796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Stream Customers Data From Cloud Files to Delta Lake using Auto Loader\n",
    "1. Read files from cloud storage using Auto Loader\n",
    "2. Transform the dataframe to add the following columns\n",
    "- file_path: Cloud File Path\n",
    "- ingestion date: Current Timestamp\n",
    "3. Write the Transformed data stream to Delta Lake Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f751aa67-6746-4bb9-b0f8-d0bbed546ece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Read files from cloud storage using Auto Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "181fc27b-8e40-4798-aeb5-4c171e2f103b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_df = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/Volumes/gizmobox/landing/operational_data/customers_autoloader/_schema\") \\\n",
    "    .load('/Volumes/gizmobox/landing/operational_data/customers_autoloader/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df8c7a3b-a600-4d45-8803-609e2a79a02c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the above shell, make note of below points\n",
    "1. All of the columns data types are strings. That is because we have not specified that we want to infer the data types, auto loader has assigned the data type for every column as string\n",
    "2. _rescued_data column is added as a string by auto loader. If it cant process some data because of schema evolution or anything like that, it will write the invalid data into the _resued_data which can be handled later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f8bf435-1462-4cd9-98b9-a26d9d72c4d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Infer the Column Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e8160c-7447-4cf2-9ac2-e3b9df82093d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_df = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/Volumes/gizmobox/landing/operational_data/customers_autoloader/_schema\") \\\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "    .load('/Volumes/gizmobox/landing/operational_data/customers_autoloader/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1b015c2-a9cc-4fdc-8913-8190318f0c85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Auto Loader tries it best to get data types from some sample but it moght not be 100% correct for some columns. As Dates and Timestamps will be wrapped in quotes, it will identify them as strings. Sometimes the column will have a mix of strings and numbers which will be identified as a string.\n",
    "To influecne auto loader to set data types for few columns, we have an option called cloudFiles.schemaHints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b505c8-3f1d-45b1-87c7-b424f81b3bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_df = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/Volumes/gizmobox/landing/operational_data/customers_autoloader/_schema\") \\\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "    .option(\"cloudFiles.schemaHints\", \"date_of_birth date, member_since date, created_timestamp timestamp\") \\\n",
    "    .load('/Volumes/gizmobox/landing/operational_data/customers_autoloader/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4e1ddae-248b-4bd2-957d-d91216238b99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Transform the dataframe to add the following columns\n",
    "- file_path: Cloud File Path\n",
    "- ingestion date: Current Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed096513-634b-4bc2-80eb-8ee2388f9540",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "customers_transformed_df = customers_df.withColumn(\"file_path\", col(\"_metadata.file_path\")) \\\n",
    "                                        .withColumn(\"ingestion_date\", current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "627e17e6-a066-4095-94eb-6bd409f38262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Write the Transformed data stream to Delta Lake Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71597ff1-156c-4677-84dd-780f297bbbeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_query = customers_transformed_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "        .option(\"checkpointLocation\", \"/Volumes/gizmobox/landing/operational_data/customers_autoloader/_checkpoint_stream\") \\\n",
    "        .toTable(\"gizmobox.bronze.customers_autoloader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5213650a-d8f9-4272-8f23-31a66ceb4b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766dd403-124c-418e-a5bb-5c547e0d2f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select *\n",
    "from gizmobox.bronze.customers_autoloader;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3fad1e8-c7e5-446e-85d3-6390fb35aeaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2320084063860221,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01. Ingest Customers Stream using Auto Loader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
